{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "musuka.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOT5qeirlIE7D45zX16SIH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kake01/Bot/blob/master/musuka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIW6f0txCZ6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ドライブをマウントする\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Jh3TLUFDiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 蛇の目インストール\n",
        "pip install janome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tzV8KDS924T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テキストデータの分割\n",
        "import sys\n",
        "import os\n",
        "import chardet\n",
        "import tensorflow as tf\n",
        "import os.path\n",
        "import pprint\n",
        "\n",
        "from os.path import join, splitext, split\n",
        "\n",
        "# 入力ディレクトリと出力ディレクトリを指定\n",
        "readdir = sys.argv[1]\n",
        "outdir =\"drive/My Drive/Bot/Serif\"\n",
        "\n",
        "# 天空の城ラピュタのテキストデータを代入\n",
        "file_data = open('drive/My Drive/Bot/TextData.txt', 'r',encoding='utf-8')\n",
        "lines = file_data.readlines()\n",
        "# #読み込んだテキストファイルを１行ずつ表示\n",
        "# for line in lines:\n",
        "\n",
        "# キャラ名を取得\n",
        "    char_name = line[:line.find(u\"「\")]\n",
        "    outfname = os.path.join(outdir, char_name + \".txt\")\n",
        "#キャラ名のファイルがあるかどうか確認\n",
        "    if os.path.exists(outfname):\n",
        "        # ある場合は上書きモードで\n",
        "        outfp = open(outfname, \"a\")\n",
        "    else:\n",
        "        # ない場合は新規作成\n",
        "        outfp = open(outfname, \"w\")\n",
        "        \n",
        "    # セリフのみ抽出\n",
        "    line_format = line[line.find(u\"「\") + 1:line.find(u\"」\")] + \"\\n\"\n",
        "    # セリフ書き込み\n",
        "    outfp.write(line_format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5iWHpmZjQAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 関数関係\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "BEGIN = '__BEGIN__'\n",
        "END = '__END__'\n",
        "\n",
        "\"\"\" 文章を3単語の組にして返す\"\"\"\n",
        "def get_three_words_list(sentence):\n",
        "  t = Tokenizer()\n",
        "  words = t.tokenize(sentence, wakati=True)\n",
        "  words = [BEGIN] + words + [END]\n",
        "  three_words_list = []\n",
        "  for i in range(len(words) - 2):\n",
        "    three_words_list.append(tuple(words[i:i+3]))\n",
        "\n",
        "  return three_words_list\n",
        "\n",
        "\"\"\" マルコス連鎖での文章生成用の辞書データを生成する \"\"\"\n",
        "def generate_markov_dict(three_words_conut):\n",
        "  markov_dict = {}\n",
        "  for three_words, count in three_words_conut.items():\n",
        "    two_words = three_words[:2]\n",
        "    next_word = three_words[2]\n",
        "    if two_words not in markov_dict:\n",
        "      markov_dict[two_words] = {'words': [], 'weights': []}\n",
        "    markov_dict[two_words]['words'].append(next_word)\n",
        "    markov_dict[two_words]['weights'].append(count)\n",
        "  \n",
        "  return markov_dict\n",
        "\n",
        "\"\"\" 最初の単語を選択するための辞書データを作成する \"\"\"\n",
        "def get_first_word_and_count(three_words_conut):\n",
        "\n",
        "  first_word_count = defaultdict(int)\n",
        "\n",
        "  for three_words, count in three_words_conut.items():\n",
        "    if three_words[0] == BEGIN:\n",
        "      next_word = three_words[1]\n",
        "      first_word_count[next_word] += count\n",
        "\n",
        "  return first_word_count\n",
        "\n",
        "\"\"\" 最初の単語を選択するための辞書データを作成する \"\"\"\n",
        "def get_first_word_and_count(three_words_conut):\n",
        "  first_word_count = defaultdict(int)\n",
        "\n",
        "  for three_words, count in three_words_conut.items():\n",
        "    if three_words[0] == BEGIN:\n",
        "      next_word = three_words[1]\n",
        "      first_word_count[next_word] += count\n",
        "\n",
        "  return first_word_count\n",
        "\n",
        "\"\"\" 最初の単語と重みのリストを作成する　\"\"\"\n",
        "def get_first_words_weights(three_words_conut):\n",
        "  first_word_count = get_first_word_and_count(three_words_conut)\n",
        "  words = []\n",
        "  weights = []\n",
        "  for word, count in first_word_count.items():\n",
        "    words.append(word)\n",
        "    weights.append(count)\n",
        "\n",
        "  return words, weights\n",
        "\n",
        "\"\"\" 入力された辞書データを元に文章を生成する \"\"\"\n",
        "def generate_text(first_words, first_weights, markov_dict):\n",
        "  first_word = random.choices(first_words, weights = first_weights)[0]\n",
        "  generate_words = [BEGIN, first_word]\n",
        "  while True:\n",
        "    pair = tuple(generate_words[-2:])\n",
        "    words = markov_dict[pair]['words']\n",
        "    weights = markov_dict[pair]['weights']\n",
        "    next_word = random.choices(words, weights=weights)[0]\n",
        "    if next_word == END:\n",
        "      break\n",
        "    generate_words.append(next_word)\n",
        "\n",
        "  return ''.join(generate_words[1:])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZNVN0lAhYUP",
        "colab_type": "code",
        "outputId": "d26cdfcf-d0a2-4fdf-b040-bca042436e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# テキストデータの前処理\n",
        "# 1文章を配列に入れる\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "import chardet\n",
        "import os.path\n",
        "import pprint\n",
        "import re\n",
        "from os.path import join, splitext, split\n",
        "\n",
        "sentences1 = []\n",
        "three_words_list = []\n",
        "\n",
        "path = 'drive/My Drive/Bot/Serif/ムスカ.txt'\n",
        "file_data = open(path, 'r',encoding='utf-8')\n",
        "\n",
        "# 1文章を配列に入れる\n",
        "for line in file_data:\n",
        "  line_format = line[:line.find(\"。\")]\n",
        "  # sentences1.append(line_format)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for sentence in tqdm(sentences1):\n",
        "  three_words_list += get_three_words_list(sentence)\n",
        "three_words_conut = Counter(three_words_list)\n",
        "\n",
        "# ムスカ生成\n",
        "markov_dict1 = generate_markov_dict(three_words_conut)\n",
        "first_words, first_weights = get_first_words_weights(three_words_conut)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:08<00:00, 14.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z4Vuh8QwNi6",
        "colab_type": "code",
        "outputId": "0a454d31-3faa-4dc9-d68e-ae67f90437a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(sentences1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['くいとめろ', 'てこずらせたな...', '制服さんの悪い癖だ', '閣下が不用意に打たれた暗号を解読されたのです', 'これは、私の機関の仕事です', 'もちろん', 'よく眠れたかな？', 'はやりの服は嫌いですか？...彼なら安心したまえ', '入りたまえ', 'すさまじい破壊力を持つ、ロボットの兵隊だよ', 'こいつは...、地上で造られたものではない', 'そこだ', '同じ印が、君の家の古い暖炉にあった', 'こいつは君の手にあるときにしか、働かない', '石は持ち主を守り、いつの日にか天空のラピュタへ帰るときの道しるべとして君に受け継がれたんだ', '君は、ラピュタを宝島か何かのように考えているのかね...', 'ラピュタはかつて、恐るべき科学力で天空にあり、全地上を支配した、恐怖の帝国だったのだ!!', '...そんなものがまだ空中をさまよっているとしたら、平和にとってどれだけ危険なことか、君にも分かるだろう', '私に協力して欲しい', '私は、手荒なことはしたくないが...あの少年の運命は君がにぎっているんだよ...', '君が協力してくれるんなら、あの少年を自由の身にしてやれるんだ', 'ウルはラピュタ語で『王』', 'パズー君', 'ラピュタの調査は、シータさんの協力で、軍が極秘に行うことになったんだ', '君も男なら、ききわけたまえ!!!!', 'これはわずかだが、心ばかりのお礼だ', '思いだしたまえ', '約束さえ果たせば、君も自由になれる', 'もう少し、時間がいります', 'すばらしい', '古文書にあった通りだ', '...ほぁぉあ!!!!', 'どんな呪文だ!!教えろその言葉を!!!!', 'ロボット？', 'ここへ来る気か？', 'すごい...!!!!', 'そぉかぁ...', 'その光だ', 'ああ、', 'ああ、ああ、ああー', 'あ゛あ゛あ゛ーーーっ、ああ゛っっ!!!!', 'あああー、あぁーー!!', '飛ぶ気か!!!!？', 'うああっっ!!!!!!', 'ああっ!!', 'あの光のさす方向に、ラピュタがあるのだ!!!!...、まだか!!早くしろ!!!!', '私はムスカ大佐だ', 'おおあ!!!!', 'どけぇ!!!!...しまった...', 'くそぉ、ゴリアテ、何をしてる!!!!', 'っあ...', '煙幕か？...', '破壊しました', '聖なる光を失わない...', 'ラピュタの位置を示している...', '将軍に伝えろ', '雲の中では無駄骨です', '手は打ちます', 'このまま進め', '光は、常に雲の渦の中心を指している', '...ラピュタは嵐の中にいる', '...これからです', 'バカどもには、ちょうどいい目くらましだ', 'この辺りだ', '...これだ', '撃つな!!', 'とらえろ!!', 'これはこれは、王女さまではないかぁ', '海賊の残りだ', 'もう一匹、その足もとに隠れているぞ', 'ラピュタの中枢だ', 'お前達はここで待て', 'ここから先は、王族しか入れない聖域なのだ', '...なんだこれは？!!', '...木の根が、こんなところまで!!!!', '...一段落したら、全て焼き払ってやる', '来たまえ、こっちだ', 'っくそぉっ!!...あった!!!!これだ!!!!', '...ここもか!!...はぁっ!!', 'くっそぉっ!!!!', '...あっ、ぐっ、...あった...', 'おおおお', '君の一族は、そんなことも忘れてしまったのかね？', '...黒い石だぁ', 'はっ、はぁっ、はぁへ!!...読める、読めるぞぉ!!', 'わたしも、古い秘密の名前を持ってるんだよ', 'わたしの名は、ロムスカ・パロ...ウル・ラピュタ', '君の一族と私の一族は、もともと一つの、王家だったのだ', '閣下', 'そんなことをせずとも', '入れますよ...', 'さあ!!何をためらうのです', 'お静かに...', '...言葉をつつしみたまえ', 'これから王国の復活を祝って、諸君にラピュタの力を見せてやろうと思ってね...', '見せてあげよう', '旧約聖書にある、ソドムとゴモラを滅ぼした、天の火だよ', '全世界は再びラピュタのもとにひれ伏すことになるだろう', '君のアホ面には、心底うんざりさせられる...', 'ん゛、あ゛あ゛っ!!!!くっ、こいつ...!!', 'ああ...、死ねぇぇぇ゛っっっ!!!!!!!!', 'っふっはっはっはっはっはっはっはっは...', '私をあまり怒らせない方がいいぞ', '当分二人っきりでここに住むのだからなぁ!!', 'ははっ...さっさと逃げればいいものを', 'はっっはっはっはっはっは、私と戦うつもりか!!', 'すばらしい...!!最高のショウだと思わんかね', '...ほぉぉ？!!ふっはっは、見ろ!!!!人がゴミのようだ!!!!っはっはっはっはっはっはっは...', 'っ、何をする!!!!', 'くそぉ...', '返したまえ...いい子だから', 'っはっはっはっは、どこへゆこうというのかね？', 'っはっはっはっはっはっはっは...', 'その石を大事に持ってろ!!小娘の命と引き替えだ!!!!', '...立て!!鬼ごっこは終わりだ!!!!', '...終点が、玉座の間だとは、上出来じゃないか', 'ラピュタは滅びぬ', '次は耳だ', '小僧', '３分間待ってやる', '時間だ!!答えを聞こう!!!!', '...んん？', 'ぉうあ゛あ゛あ゛あ゛ーーーーっ!!!!!!があっっ!!!!!!', 'へぁぁぁーー、はぁぁ、目がぁー!!目がぁーーぁぁぁぁぁぁぁ!!!!', 'ああ、ああ、目が、あああぁぁぁぁあーー!!...あ゛あ゛あ゛あ゛...!!!!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJRk1uHRiNL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(sentences1)\n",
        "# Janomeのロード\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "# Tokenneizerインスタンスの生成 \n",
        "t = Tokenizer()\n",
        "\n",
        "# テキストを引数として、形態素解析の結果、名詞・動詞・形容詞の原形のみを配列で抽出する関数を定義 \n",
        "def extract_words(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    return [token.base_form for token in tokens \n",
        "        if token.part_of_speech.split(',')[0] in['名詞', '動詞','形容詞']]\n",
        "\n",
        "# 分かち書きの配列生成\n",
        "for line in file_data:\n",
        "  line_format = line[:line.find(\"。\")]\n",
        "  word_list = [extract_words(sentence) for sentence in line_format]\n",
        "\n",
        "# 結果の一部を確認\n",
        "print(word_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5JTDlSS4J-0_",
        "colab": {}
      },
      "source": [
        "# 文字列を生成する\n",
        "for _ in range(10):\n",
        "  text = generate_text(first_words, first_weights, markov_dict1)\n",
        "  print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}